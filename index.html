<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
  <meta name=viewport content="width=800">
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */

    a {
      color: #1772d0;
      text-decoration: none;
    }

    a:focus,
    a:hover {
      color: #f09228;
      text-decoration: none;
    }

    body,
    td,
    th,
    tr,
    p,
    a {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px
    }

    strong {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px;
    }

    heading {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 22px;
    }

    papertitle {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px;
      font-weight: 700
    }

    name {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 32px;
    }

    .one {
      width: 160px;
      height: 160px;
      position: relative;
    }

    .two {
      width: 160px;
      height: 160px;
      position: absolute;
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }

    .fade {
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }

    span.highlight {
      background-color: #ffffd0;
    }
  </style>
  <link rel="icon" type="image/png" href="images/seal_icon.png">
  <title>Sudeep Dasari</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
</head>

<body>
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
      <td>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="67%" valign="middle">
              <p align="center">
                <name>Sudeep Dasari</name>
              </p>
              <p>I'm a PhD student at the <a href="https://www.ri.cmu.edu/">Robotics Institute</a> in <a href="https://www.cs.cmu.edu/">Carnegie Mellon's School of Computer Science</a>. I aspire to build scalable robotic learning algorithms, which can parse the visual world and enable autonomous agents to perform complex tasks in diverse environments. I am advised by <a href="http://www.cs.cmu.edu/~abhinavg/">Professor Abhinav Gupta</a>.</p>
              <p>
                In a prior life, I was an undergraduate student at <a href="https://eecs.berkeley.edu/">UC Berkeley</a> - where I work with <a href="https://people.eecs.berkeley.edu/~svlevine/"> Professor Sergey Levine</a> on deep reinforcement learning/machine learning research. I also worked at <a href="https://www.lanl.gov/"> Los Alamos National Laboratory </a> with <a href="https://www.linkedin.com/in/davidmascarenasengr"> Dr. David Mascare&ntilde;as</a> on cyber-phsyical systems research.
              </p>
              <p align=center>
                <a href="mailto:sdasari@cs.cmu.edu">Email</a> &nbsp/&nbsp
                <a href="https://drive.google.com/file/d/1-lqvsm2JAaFl5ZzUXQJPeuEPklMJyI9b/view?usp=sharing">CV</a> &nbsp/&nbsp
                <a href="https://github.com/SudeepDasari"> GitHub </a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=NpOg5soAAAAJ&hl=en"> Google Scholar </a>
              </p>
            </td>
            <td width="33%">
              <img src="misc/prof.jpg" width=200>
            </td>
          </tr>
        </table>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <heading>Research</heading>
              <p>
                I'm interested in computer vision, machine learning, robotics, statistics, and generative modelling. I aspire to push the boundaries of what robots can do with visual data given minimal supervision, and create new ways for them to interact more naturally with humans.
              </p>
            </td>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tr>
                <td width="25%">
                    <img src='research/RoboNet/sample.gif'  width=180>
  
                </td>
                <td valign="top" width="75%">
                  <a href="https://arxiv.org/abs/1910.11215">
                    <papertitle>RoboNet: Large-Scale Multi-Robot Learning</papertitle>
                  </a>
                  <br>
                  <strong>Sudeep Dasari</strong>,
                  <a href="https://febert.github.io/">Frederik Ebert</a>,
                  <a href="https://s-tian.github.io/">Stephen Tian</a>,
                  <a href="https://cs.stanford.edu/~surajn/">Suraj Nair</a>,
                  <a href="https://bucherb.github.io/">Bernadette Bucher</a>,
                  <a href="https://sites.google.com/view/karlschmeckpeper">Karl Schmeckpeper</a>,
                  <a href="https://www.seas.upenn.edu/~sidsingh/">Siddharth Singh</a>,
                  <a href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a>,
                  <a href="https://ai.stanford.edu/~cbfinn/"> Chelsea Finn</a>
                  <br>
                  <em>3rd Conference on Robotic Learning</em>, 2019
                  <br>
                  <a href="https://www.robonet.wiki/">project page</a>,
                  <a href="https://github.com/SudeepDasari/RoboNet">code</a>,
                  <a href="https://www.technologyreview.com/s/614668/welcome-to-robot-university-only-robots-need-apply/">Press Coverage</a>
                  <p></p>
                  <p> We demonstrate that a large and diverse data-set of robotic experience (video + robot telemetry) can enable robot learning algorithms to learn new skills in new environments, faster than training from scratch. </p>
                </td>
              </tr>

          <tr>
              <td width="25%">
                  <img src='research/foresight/short_fold.gif'  width=180>

              </td>
              <td valign="top" width="75%">
                <a href="https://arxiv.org/abs/1812.00568">
                  <papertitle>Visual Foresight: Model-Based Deep Reinforcement Learning for Vision-Based Robotic Control</papertitle>
                </a>
                <br>
                <a href="https://febert.github.io/">Frederik Ebert*</a>,
                <a href="http://people.eecs.berkeley.edu/~cbfinn/"> Chelsea Finn*</a>,
                <strong>Sudeep Dasari</strong>,
                Annie Xie,
                <a href="http://people.eecs.berkeley.edu/~alexlee_gk/">Alex X. Lee</a>,
                <a href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a>
                <br>
                <em>arXiv Preprint</em>, 2018
                <br>
                <a href="https://sites.google.com/view/visualforesight">project page</a>,
                <a href="https://github.com/SudeepDasari/visual_foresight">code</a>,
                <a href="https://bair.berkeley.edu/blog/2018/11/30/visual-rl/">blog post</a>
                <p></p>
                <p>We demonstrate that video prediction models - generative models trained on autonomously collected data of robots interacting with their environment - can be used for vision based robotic control. </p>
              </td>
            </tr>

            <tr>
                <td width="25%">
                    <img src='research/robustness/retry.gif'  width=180>

                </td>
                <td valign="top" width="75%">
                  <a href="https://arxiv.org/pdf/1810.03043">
                    <papertitle>Robustness via Retrying</papertitle>
                  </a>
                  <br>
                  <a href="https://febert.github.io/">Frederik Ebert</a>,
                  <strong>Sudeep Dasari</strong>,
                  <a href="http://people.eecs.berkeley.edu/~alexlee_gk/">Alex X. Lee</a>,
                  <a href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a>,
                  <a href="http://people.eecs.berkeley.edu/~cbfinn/"> Chelsea Finn</a>
                  <br>
                  <em>2nd Conference on Robotic Learning</em>, 2018
                  <br>
                  <a href="https://sites.google.com/view/robustness-via-retrying">project page</a>,
                  <a href="https://github.com/febert/robustness_via_retrying">code</a>
                  <p></p>
                  <p> While visual prediction planners can acheive a wide variety of manipulation tasks, they often fail by loosing track of the object. We propose a self-supervised registration algorithm which allows even imperfect planners to retry continuously until they succeed. </p>
                </td>
              </tr>
              <tr>
                  <td width="25%">
                      <img src='research/DAML/human_imitation_small.gif'  width=180>

                  </td>
                  <td valign="top" width="75%">
                    <a href="https://arxiv.org/abs/1802.01557">
                      <papertitle>Domain Adaptive Meta-Learning</papertitle>
                    </a>
                    <br>
                    <a href="https://tianheyu927.github.io/"> Tianhe Yu*</a>,
                    <a href="http://people.eecs.berkeley.edu/~cbfinn/"> Chelsea Finn*</a>,
                    Annie Xie,
                    <strong>Sudeep Dasari</strong>,
                    <a href="http://tianhaozhang.com/">Tianhao Zhang</a>,
                    <a href="http://people.eecs.berkeley.edu/~pabbeel/">Pieter Abbeel</a>,
                    <a href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a>,
                    <br>
                    <em>Robotic Science and Systems</em>, 2018
                    <br>
                    <a href="https://sites.google.com/view/daml">project page</a>,
                    <a href="https://github.com/tianheyu927/mil">code</a>
                    <p></p>
                    <p> Domain Adaptive Meta-Learning allows for one-shot learning under domain shift. Using our policy, robots can to learn to manipulate unseen objects by referring to a single video demonstration of a human performing a task with said object. </p>
                  </td>
                </tr>

                <tr>
                    <td width="25%">
                        <img src='research/DAML/demo.jpg'  width=180>

                    </td>
                    <td valign="top" width="75%">
                      <a href="http://rail.eecs.berkeley.edu/nips_demo.html">
                        <papertitle>Deep Robotic Learning using Visual Imagination & Meta-Learning</papertitle>
                      </a>
                      <br>
                      <a href="http://people.eecs.berkeley.edu/~cbfinn/"> Chelsea Finn</a>,
                      <strong>Sudeep Dasari*</strong>,
                      Annie Xie*,
                      <a href="https://febert.github.io/">Frederik Ebert</a>,
                      <a href="https://tianheyu927.github.io/"> Tianhe Yu</a>,
                      <a href="http://people.eecs.berkeley.edu/~pabbeel/">Pieter Abbeel</a>,
                      <a href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a>
                      <br>
                      <em>Demonstration at Neural Information Processing Seminar</em>, 2017
                      <br>
                      <a href="http://rail.eecs.berkeley.edu/nips_demo.html">project page</a>,
                      <a href="https://www.nytimes.com/interactive/2018/07/30/technology/robot-hands.html">press coverage</a>
                      <p></p>
                      <p> A key, unsolved challenge for learning with real robotic systems is the ability to acquire vision-based behaviors from raw RGB images. We present two approaches to this goal that we demonstrate: first, learning task-agnostic visual models for planning and second, learning to quickly adapt to new objects and environments using meta-imitation learning. </p>
                    </td>
                  </tr>

                  <tr>
                      <td width="25%">
                          <img src='research/LANL/camera.png'  width=180>

                      </td>
                      <td valign="top" width="75%">
                        <a href="https://www.springerprofessional.de/en/light-field-imaging-of-three-dimensional-structural-dynamics/15801000">
                          <papertitle>Light Field Imaging of Three-Dimensional Structural Dynamics</papertitle>
                        </a>
                        <br>
                        Benjamin Chesebrough,
                        <strong>Sudeep Dasari</strong>,
                        <a href="https://www.linkedin.com/in/andre-green-los-alamos"> Andre Green</a>,
                        <a href="https://www.linkedin.com/in/yongchao-yang-63717b9a"> Yongchao Yang</a>,
                        <a href="https://jacobsschool.ucsd.edu/faculty/faculty_bios/index.sfe?fmp_recid=270"> Charles R. Farrar </a>,
                        <a href="https://www.linkedin.com/in/davidmascarenasengr"> David Mascare&ntilde;as</a>
                        <br>
                        <em>Society for Experimental Mechanics: IMAC</em>, 2018
                        <br>
                        <p></p>
                        <p> Cameras offer a way to perform low cost and non-invasive damage detection by analyzing a structure's vibrations. To acheive this goal, we use a light field imager to create a 3D point cloud of vibrating cantilever beams, and then use Principal Component Analysis and Blind Source Seperation to find the beam's mode shapes and frequencies. </p>
                      </td>
                    </tr>

                    <tr>
                        <td width="25%">
                            <img src='research/LANL/spectrum.png'  width=180>

                        </td>
                        <td valign="top" width="75%">
                          <a href="https://journals.sagepub.com/doi/abs/10.1177/1045389X17754271">
                            <papertitle>A framework for the identification of full-field structural dynamics using sequences of images in the presence of non-ideal operating conditions</papertitle>
                          </a>
                          <br>
                          <strong>Sudeep Dasari</strong>,
                          <a href="https://www.linkedin.com/in/charles-dorn-8708ab79"> Charles Dorn</a>,
                          <a href="https://www.linkedin.com/in/yongchao-yang-63717b9a"> Yongchao Yang</a>,
                          <a href="https://at.linkedin.com/in/dramylarson"> Amy Larson </a>,
                          <a href="https://www.linkedin.com/in/davidmascarenasengr"> David Mascare&ntilde;as</a>
                          <br>
                          <em>Journal of Intelligent Material Systems and Structures</em>, 2017
                          <br>
                          <p></p>
                          <p> Recent developments in analyzing structures with raw video streams has great potential for reducing the resources and time required for performing operational modal analysis at very high spatial resolution. We propose a key-point based registration technique to extend this method to settings with large rigid body motion (e.g drone footage). </p>
                        </td>
                      </tr>

                      <tr>
                          <td width="25%">
                              <img src='research/LANL/retina.png'  width=180>

                          </td>
                          <td valign="top" width="75%">
                            <a href="https://ascelibrary.org/doi/abs/10.1061/%28ASCE%29EM.1943-7889.0001449">
                              <papertitle>Efficient Full-Field Vibration Measurements and Operational Modal Analysis Using Neuromorphic Event-Based Imaging</papertitle>
                            </a>
                            <br>
                            <a href="https://www.linkedin.com/in/charles-dorn-8708ab79"> Charles Dorn</a>,
                            <strong>Sudeep Dasari</strong>,
                            <a href="https://www.linkedin.com/in/yongchao-yang-63717b9a"> Yongchao Yang</a>,
                            <a href="https://jacobsschool.ucsd.edu/faculty/faculty_bios/index.sfe?fmp_recid=270"> Charles R. Farrar </a>,
                            Garrett Kenyon,
                            Paul Welch,
                            <a href="https://www.linkedin.com/in/davidmascarenasengr"> David Mascare&ntilde;as</a>
                            <br>
                            <em>Journal of Engineering Mechanics</em>, 2017
                            <br>
                            <p></p>
                            <p> We demonstrate that asynchronous silicon retina imagers can capture structural motion on the microsecond scale in an extremely data-efficient manner. </p>
                          </td>
                        </tr>
        </table>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td>
              <heading>Course Projects</heading>
            </td>
          </tr>
        </table>
        <table width="100%" align="center" border="0" cellpadding="20">
          <tr>
            <td width="25%">
            <a href="https://funwwords.wordpress.com/"> <img src="class_projects/words/review.png" width="200"> </a>
            <td width="75%" valign="top">
              <p>
                <a href="https://funwwords.wordpress.com/">
                  <papertitle>Fun With Words: Generating Text with Context</papertitle>
                </a>
                <br>
                <strong>Sudeep Dasari</strong>, <a href="https://www.jzhao.me/">Hankun (Jerry) Zhao</a>, <a href="https://www.linkedin.com/in/william-zhao-3a0588104"> William Zhao</a>, 2018
                <p>
                  <br>State of the art machine translation networks can be used for conditional text generation. We leverage the Yelp dataset to train models which generate sensible 1 - 5 star reviews of resteraunts given only the establishment's name.
                </p>
              </p>
            </td>
          </tr>
          <tr onmouseout="dragon_stop()" onmouseover="dragon_start()">
            <td width="25%">
              <div id='dragon_anim' class='one'>
                <div id='dragon_still' class='two'><img src="class_projects/Mirage/images/dragon128.png" width="200"></div>
                <a href="class_projects/Mirage/index.html"><img src="class_projects/Mirage/images/dragon512.png" width="200"></a>
              </div>

              <script type="text/javascript">
                function dragon_start() {
                  document.getElementById('dragon_still').style.opacity = '1';
                }

                function dragon_stop() {
                  document.getElementById('dragon_still').style.opacity = '0';
                }
                eccv12_stop()
              </script>
            <td width="75%" valign="top">
              <p>
                <a href="class_projects/Mirage/index.html">
                  <papertitle>Non-Linear Raytracing</papertitle>
                </a>
                <br>
                <strong>Sudeep Dasari</strong>, <a href="https://jenniferwenhsu.github.io/">Jennifer Wen Hsu</a>, 2017
                <p>
                  <br> Most ray-tracing algorithms assume that light travels in a straight line from the camera into the scene. However, there are many cases where a light may travel a non-linear path (e.g <a href="https://en.wikipedia.org/wiki/Mirage"> mirage</a>). We demonstrate non-linear raytracing can correctly render scenes with mirages and fata-morgana, as well as trace light passing through Gradient Optic Index Lenses.
                </p>
              </p>
            </td>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td>
              <br>
              <p align="right">
                <font size="2">
                  Template courtesy of <a href="https://jonbarron.info/"><strong>here</strong></a>

                  </font>
              </p>
            </td>
          </tr>
        </table>
        <script type="text/javascript">
          var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
          document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
        </script>
        <script type="text/javascript">
          try {
            var pageTracker = _gat._getTracker("UA-7580334-1");
            pageTracker._trackPageview();
          } catch (err) {}
        </script>
        </td>
    </tr>
  </table>
</body>

</html>
