<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
  <meta name=viewport content="width=800">
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */

    a {
      color: #1772d0;
      text-decoration: none;
    }

    a:focus,
    a:hover {
      color: #f09220;
      text-decoration: none;
    }

    body,
    td,
    th,
    tr,
    p,
    a {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px
    }

    strong {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px;
    }

    heading {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 22px;
    }

    papertitle {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px;
      font-weight: 700
    }

    name {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 32px;
    }

    .one {
      width: 160px;
      height: 160px;
      position: relative;
    }

    .two {
      width: 160px;
      height: 160px;
      position: absolute;
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }

    .fade {
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }

    span.highlight {
      background-color: #ffffd0;
    }
  </style>
  <link rel="icon" type="image/png" href="images/berk_logo.png">
  <title>Murtaza Dalal</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
</head>

<body>
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
      <td>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="67%" valign="middle">
              <p align="center">
                <name>Murtaza Dalal</name>
              </p>
               <p>
                Hi! I am an undergraduate student at <a href="https://eecs.berkeley.edu/">UC Berkeley</a> majoring in Electrical Engineering and Computer Science and working with <a href="https://people.eecs.berkeley.edu/~svlevine/"> Professor Sergey Levine</a> on deep reinforcement learning/robotics research. I also worked at <a href="https://x.company/"> (Google) X Robotics </a> with <a href="https://www.linkedin.com/in/daniel-kappler-a20b337b/"></a> Dr. Daniel Kappler on a reinforcement learning and robotics project during Summer 2019. 
              </p>


              <p> Feel free to contact me via email! You can reach me at mdalal -at- berkeley dot edu</p>


              <p align=center>
                <a href="mailto:mdalal@berkeley.edu">Email</a> &nbsp/&nbsp
                <a href="https://drive.google.com/file/d/14Jn6rd7IrXrG__q3JKD9ULO6jjdLYSc8/view?usp=sharing">CV</a> &nbsp/&nbsp
                <a href="https://github.com/mdalal2020"> GitHub </a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=M8zPKWgAAAAJ&hl=en"> Google Scholar </a>
              </p>
            </td>
            <td width="33%">
              <img src="images/download.png">
            </td>
          </tr>
        </table>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <heading>Research</heading>
              <p>
                I am interested in developing intelligent, sample efficient, adaptable robotics systems leveraging techniques from deep reinforcement learning, machine learning, optimization, control and computer vision. 
              </p>
            </td>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

          <tr>
            <td width="25%"><img src="images/skewfit_vitchyr.png" width="240" height="120" style="border-style: none">
              <td width="75%" valign="top">
                <p>
                  <a href="https://arxiv.org/pdf/1903.03698.pdf">
                    <papertitle>Skew-Fit: State-Covering Self-Supervised Reinforcement Learning</papertitle>
                  </a>
                  <br>
                  <a href="http://people.eecs.berkeley.edu/~vitchyr/">Vitchyr H. Pong*</a>, <strong> Murtaza Dalal* </strong>, Steven Lin*, <a href="http://ashvin.me/">Ashvin Nair</a>, <a href="https://shikharbahl.github.io/"> Shikhar Bahl</a>, <a href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a>
                  <br>
                  <em> International Conference on Learning Representations (ICLR) in submission, </em> 2020  <font color="red"></font>
                  <br>
                  [<a href="https://arxiv.org/pdf/1903.03698.pdf">arXiv</a>]
                  [<a href="https://sites.google.com/view/skew-fit">site</a>]
                  [<a href="https://github.com/vitchyr/rlkit">code</a>]
                </p>
                <p> In standard reinforcement learning, each new skill requires a manually-designed
                  reward function, which takes considerable manual effort and engineering. Selfsupervised goal setting has the potential to automate this process, enabling an agent
                  to propose its own goals and acquire skills that achieve these goals. However, such
                  methods typically rely on manually-designed goal distributions, or heuristics to
                  force the agent to explore a wide range of states. We propose a formal exploration
                  objective for goal-reaching policies that maximizes state coverage. We show that
                  this objective is equivalent to maximizing the entropy of the goal distribution
                  together with goal reaching performance, where goals correspond to entire states.
                  We present an algorithm called Skew-Fit for learning such a maximum-entropy
                  goal distribution, and show that under certain regularity conditions, our method
                  converges to a uniform distribution over the set of valid states, even when we do not
                  know this set beforehand. Skew-Fit enables self-supervised agents to autonomously
                  choose and practice reaching diverse goals. Our experiments show that it can learn
                  a variety of manipulation tasks from images, including opening a door with a real
                  robot, entirely from scratch and without any manually-designed reward function.
                </p>
                <br>
                </p>
              </td>
          </tr>

          <tr>
              <td width="25%"><img src="images/rig_vitch.png" width="200"  style="border-style: none">
                <td width="75%" valign="top">
                  <p>
                    <a href="http://papers.nips.cc/paper/8132-visual-reinforcement-learning-with-imagined-goals.pdf">
                      <papertitle>Visual Reinforcement Learning with Imagined Goals</papertitle>
                    </a>
                    <br>
                    <a href="http://ashvin.me/">Ashvin Nair*</a>, <a href="http://people.eecs.berkeley.edu/~vitchyr/">Vitchyr H. Pong*</a>, <strong> Murtaza Dalal </strong>, <a href="https://shikharbahl.github.io/"> Shikhar Bahl</a>, Steven Lin, <a href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a>
                    <br>
                    <em> Advances in Neural Information Processing Systems (NeurIPS), </em> 2018  <font color="red"><strong>(Spotlight Presentation)</strong></font>
                    <br>
                    [<a href="https://arxiv.org/pdf/1807.04742.pdf">arXiv</a>]
                    [<a href="https://sites.google.com/site/visualrlwithimaginedgoals/">site</a>]
                    [<a href="https://github.com/vitchyr/rlkit">code</a>]
                    [<a href="https://bair.berkeley.edu/blog/2018/09/06/rig/">blog</a>]
                  </p>
                  <p> For an autonomous agent to fulfill a wide range of user-specified goals at test time, it must be able to learn broadly applicable and general-purpose skill repertoires. Furthermore, to provide the requisite level of generality, these skills must handle raw sensory input such as images. In this paper, we propose an algorithm that acquires such general-purpose skills by combining unsupervised representation learning and reinforcement learning of goal-conditioned policies. Since the particular goals that might be required at test-time are not known in advance, the agent performs a self-supervised "practice" phase where it imagines goals and attempts to achieve them. We learn a visual representation with three distinct purposes: sampling goals for self-supervised practice, providing a structured transformation of raw sensory inputs, and computing a reward signal for goal reaching. We also propose a retroactive goal relabeling scheme to further improve the sample-efficiency of our method. Our off-policy algorithm is efficient enough to learn policies that operate on raw image observations and goals for a real-world robotic system, and substantially outperforms prior techniques.
                  </p>
                  <br>
                  </p>
                </td>
            </tr>
            <tr>
              <td width="25%"><img src="images/composable.png" width="200" style="border-style: none"></td>
              <td width="75%" valign="top">
              <p>
              <a href="https://arxiv.org/abs/1803.06773">
                <papertitle>
                    Composable Deep Reinforcement Learning for Robotic Manipulation
                </papertitle>
                </a>
                <br>
                <a href="https://people.eecs.berkeley.edu/~haarnoja/">
                    Tuomas Haarnoja*
                </a>,
                <a href="http://people.eecs.berkeley.edu/~vitchyr/">Vitchyr H. Pong</a>,
                Aurick Zhou,
                <strong> Murtaza Dalal </strong>,
                <a href="https://people.eecs.berkeley.edu/~pabbeel/">Pieter
                    Abbeel</a>,
                <a href="https://people.eecs.berkeley.edu/~svlevine/">
                    Sergey Levine
                </a>
                <br>
                <em>
                    International Conference on Robotics and Automation (ICRA), 2018.
                </em>
                <br>
                [<a href="https://arxiv.org/pdf/1803.06773.pdf">arXiv</a>]
                [<a href="https://sites.google.com/view/composing-real-world-policies">site</a>]
                [<a href="https://github.com/haarnoja/softqlearning/">code</a>]
              </p>
              <p>
              Model-free deep reinforcement learning has been shown to exhibit good
              performance in domains ranging from video games to simulated robotic
              manipulation and locomotion. However, model-free methods are known to
              perform poorly when the interaction time with the environment is
              limited, as is the case for most real-world robotic tasks. In this
              paper, we study how maximum entropy policies trained using soft
              Q-learning can be applied to real-world robotic manipulation. The
              application of this method to real-world manipulation is facilitated by
              two important features of soft Q-learning. First, soft Q-learning can
              learn multimodal exploration strategies by learning policies represented
              by expressive energy-based models. Second, we show that policies learned
              with soft Q-learning can be composed to create new policies, and that
              the optimality of the resulting policy can be bounded in terms of the
              divergence between the composed policies.
              </p>
              </td>
            </tr>
            <tr>
              <td width="25%"><img src="images/ant-smaller-2.gif" width="200" style="border-style: none"></td>
              <td width="75%" valign="top">
              <p>
              <a href="https://arxiv.org/pdf/1802.09081.pdf">
                <papertitle>Temporal Difference Models: Model-Free Deep RL for Model-Based Control</papertitle>
                </a>
                <br>
                <a href="http://people.eecs.berkeley.edu/~vitchyr/">Vitchyr H. Pong*</a>,
                <a href="http://sg717.user.srcf.net/">Shixiang Gu*</a>,
                <strong>Murtaza Dalal</strong>,
                <a href="https://people.eecs.berkeley.edu/~svlevine/">
                    Sergey Levine
                </a>
                <br>
                <em>
                    International Conference on Learning Representations (ICLR), 2018.
                </em>
                <br>
                [<a href="https://arxiv.org/abs/1802.09081">arXiv</a>]
                [<a href="https://github.com/vitchyr/rlkit">code</a>]
                [<a href="https://bair.berkeley.edu/blog/2018/04/26/tdm/">blog</a>]
              </p>
              <p>
              Model-free reinforcement learning (RL) is a powerful, general tool for
              learning complex behaviors. However, its sample efficiency is often
              impractical large for solving challenging real-world problems, even with 
              off-policy algorithms such as Q-learning.
              We introduce temporal difference models (TDMs), a family of
              goal-conditioned value functions that can be trained
              with model-free learning and used for model-based control.
              TDMs combine the benefits of model-free and model-based RL: they
              leverage the rich information in state transitions to learn very
              efficiently, while still attaining asymptotic performance that exceeds
              that of direct model-based RL methods.
              </p>
              </td>
            </tr>

        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td>
              <heading>Teaching</heading>
            </td>
          </tr>
        </table>
        <table width="100%" align="center" border="0" cellpadding="20">
          <tr>
            <td width="25%"><img src="images/cs188_bot.png" width="160" height="160"></td>
            <td width="75%" valign="center">
              <p>
                <a href="https://inst.eecs.berkeley.edu/~cs188/sp19/">
                  <papertitle>CS188 - Spring 2019 (uGSI)</papertitle>
                </a>
              </p>
            </td>
          </tr>
        </table>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td>
              <br>
              <p align="right">
                <font size="2">
                    Credit to this great <a href="https://github.com/jonbarron/jonbarron_website"><strong>repo</strong></a> for providing the source code for the website!
                  </font>
              </p>
            </td>
          </tr>
        </table>
        <script type="text/javascript">
          var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
          document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
        </script>
        <script type="text/javascript">
          try {
            var pageTracker = _gat._getTracker("UA-7580334-1");
            pageTracker._trackPageview();
          } catch (err) {}
        </script>
        </td>
    </tr>
  </table>
</body>

</html>
