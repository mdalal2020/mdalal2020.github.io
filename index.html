<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
  <meta name=viewport content="width=800">
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */

    a {
      color: #1772d0;
      text-decoration: none;
    }

    a:focus,
    a:hover {
      color: #f09220;
      text-decoration: none;
    }

    body,
    td,
    th,
    tr,
    p,
    a {
      font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
      font-size: 16px;
      font-weight: 400
    }

    strong {
      font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
      font-size: 16px;
      font-weight: 600
    }

    heading {
      font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
      font-size: 17px;
      font-weight: 600
    }

    papertitle {
      font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
      font-size: 16px;
      font-weight: 600
    }

    name {
      font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
      font-size: 32px;
      font-weight: 400
    }

    .one {
      width: 160px;
      height: 160px;
      position: relative;
    }

    .two {
      width: 160px;
      height: 160px;
      position: absolute;
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }

    .fade {
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }

    span.highlight {
      background-color: #ffffd0;
    }
  </style>
  <link rel="icon" type="image/png" href="images/ri_logo.png">
  <title>Murtaza Dalal</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
</head>

<body>
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
      <td>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="67%" valign="middle">
              <p align="center">
                <name>Murtaza Dalal</name>
              </p>
               <p>
                Hi! I am a Robotics PhD Student in the <a href="https://www.ri.cmu.edu/">Robotics Institute</a> at <a href="https://www.cmu.edu/">Carnegie Mellon University</a>. I am advised by <a href="https://www.cs.cmu.edu/~rsalakhu/">Professor Ruslan Salakhutdinov</a> and my research work is supported by the <a href="https://www.nsfgrfp.org/"> National Science Foundation Graduate Research Fellowship</a>. 
              </p>
              <p>

                Previously, I was an undergraduate student at <a href="https://eecs.berkeley.edu/">UC Berkeley</a> where I received my BS in Electrical Engineering and Computer Science. I was an undergraduate researcher in the <a href="https://bair.berkeley.edu/">Berkeley Artificial Intelligence and Research Lab</a>, working with <a href="https://people.eecs.berkeley.edu/~svlevine/">Professor Sergey Levine</a>, <a href="http://people.eecs.berkeley.edu/~vitchyr/">Vitchyr Pong</a> and <a href="https://ashvin.me/">Ashvin Nair</a> on deep reinforcement learning and robotics research. I also interned at <a href="https://x.company/">(Google) X Robotics</a> where I worked with <a href="https://www.linkedin.com/in/daniel-kappler-a20b337b/">Daniel Kappler</a> on a project at the intersection of reinforcement learning, robotics and meta-learning.
              </p>


              <p> Feel free to contact me via email! You can reach me at mdalal -at- andrew dot cmu dot edu</p>


              <p align=center>
                <a href="mailto:mdalal@andrew.cmu.edu"> Email </a> &nbsp/&nbsp
                <a href="https://drive.google.com/file/d/14Jn6rd7IrXrG__q3JKD9ULO6jjdLYSc8/view?usp=sharing"> CV </a> &nbsp/&nbsp
                <a href="https://github.com/mihdalal"> GitHub </a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=5dBp2f4AAAAJ&hl=en"> Google Scholar </a> &nbsp/&nbsp
                <a href="https://twitter.com/mihdalal"> Twitter </a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/murtaza-dalal/"> LinkedIn </a>
              </p>
            </td>
            <td width="32%" valign="top"><a href="images/profile_cmu.png"><img src="images/profile_cmu.png" width="100%" style="border-radius:15px"></a>
          </tr>
        </table>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <heading>Research</heading>
              <p>
                I am interested in developing intelligent, efficient, adaptable robotics systems by leveraging techniques from deep reinforcement learning, machine learning, optimization, control and computer vision. 
              </p>
            </td>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

          <tr>
            <td width="25%"><img src="images/awac.png" width="240" height="120" style="border-style: none">
              <td width="75%" valign="top">
                <p>
                  <a href="https://arxiv.org/pdf/2006.09359.pdf">
                    <papertitle>Accelerating Online Reinforcement Learning with Offline Datasets</papertitle>
                  </a>
                  <br>
                  <a href="http://people.eecs.berkeley.edu/~vitchyr/">Vitchyr H. Pong*</a>, <strong>Murtaza Dalal*</strong>, Steven Lin*, <a href="http://ashvin.me/">Ashvin Nair</a>, <a href="https://shikharbahl.github.io/">Shikhar Bahl</a>, <a href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a>
                  <br>
                  <em> Advances in Neural Information Processing Systems (NeurIPS), 2020, in submission </em>  <font color="red"></font>
                  <br>
                  [<a href="hhttps://arxiv.org/pdf/2006.09359.pdf">arXiv</a>]
                  [<a href="https://awacrl.github.io/">site</a>]
                  [<a href="https://github.com/vitchyr/rlkit">code</a>]
                  [<a href="https://bair.berkeley.edu/blog/2020/09/10/awac">blog</a>]
                </p>
                <!-- <p> Reinforcement learning provides an appealing formalism for learning control
                    policies from experience. However, the classic active formulation of reinforcement
                    learning necessitates a lengthy active exploration process for each behavior, making
                    it difficult to apply in real-world settings. If we can instead allow reinforcement
                    learning to effectively use previously collected data to aid the online learning
                    process, where the data could be expert demonstrations or more generally any
                    prior experience, we could make reinforcement learning a substantially more
                    practical tool. While a number of recent methods have sought to learn offline from
                    previously collected data, it remains exceptionally difficult to train a policy with
                    offline data and improve it further with online reinforcement learning. In this paper
                    we systematically analyze why this problem is so challenging, and propose a novel
                    algorithm that combines sample-efficient dynamic programming with maximum
                    likelihood policy updates, providing a simple and effective framework that is able to
                    leverage large amounts of offline data and then quickly perform online fine-tuning
                    of reinforcement learning policies. We show that our method enables rapid learning
                    of skills with a combination of prior demonstration data and online experience
                    across a suite of difficult dexterous manipulation and benchmark tasks.
                </p> -->
                <br>
                </p>
              </td>
          </tr>

          <tr>
            <td width="25%"><img src="images/skewfit_vitchyr.png" width="240" height="120" style="border-style: none">
              <td width="75%" valign="top">
                <p>
                  <a href="https://arxiv.org/pdf/1903.03698.pdf">
                    <papertitle>Skew-Fit: State-Covering Self-Supervised Reinforcement Learning</papertitle>
                  </a>
                  <br>
                  <a href="http://people.eecs.berkeley.edu/~vitchyr/">Vitchyr H. Pong*</a>, <strong>Murtaza Dalal*</strong>, Steven Lin*, <a href="http://ashvin.me/">Ashvin Nair</a>, <a href="https://shikharbahl.github.io/">Shikhar Bahl</a>, <a href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a>
                  <br>
                  <em> International Conference on Machine Learning (ICML), </em> 2020  <font color="red"></font>
                  <br>
                  [<a href="https://arxiv.org/pdf/1903.03698.pdf">arXiv</a>]
                  [<a href="https://sites.google.com/view/skew-fit">site</a>]
                  [<a href="https://github.com/vitchyr/rlkit">code</a>]
                </p>
                <!-- <p> In standard reinforcement learning, each new skill requires a manually-designed
                  reward function, which takes considerable manual effort and engineering. Selfsupervised goal setting has the potential to automate this process, enabling an agent
                  to propose its own goals and acquire skills that achieve these goals. However, such
                  methods typically rely on manually-designed goal distributions, or heuristics to
                  force the agent to explore a wide range of states. We propose a formal exploration
                  objective for goal-reaching policies that maximizes state coverage. We show that
                  this objective is equivalent to maximizing the entropy of the goal distribution
                  together with goal reaching performance, where goals correspond to entire states.
                  We present an algorithm called Skew-Fit for learning such a maximum-entropy
                  goal distribution, and show that under certain regularity conditions, our method
                  converges to a uniform distribution over the set of valid states, even when we do not
                  know this set beforehand. Skew-Fit enables self-supervised agents to autonomously
                  choose and practice reaching diverse goals. Our experiments show that it can learn
                  a variety of manipulation tasks from images, including opening a door with a real
                  robot, entirely from scratch and without any manually-designed reward function.
                </p> -->
                <br>
                </p>
              </td>
          </tr>

          <tr>
            <td width="25%"><img src="images/mili.png" width="240" height="120" style="border-style: none">
              <td width="75%" valign="top">
                <p>
                  <a href="https://arxiv.org/abs/2003.02636">
                    <papertitle>Scalable Multi-Task Imitation Learning with Autonomous Improvement</papertitle>
                  </a>
                  <br>
                  <a href="http://people.eecs.berkeley.edu/~vitchyr/">Vitchyr H. Pong*</a>, <strong>Murtaza Dalal*</strong>, Steven Lin*, <a href="http://ashvin.me/">Ashvin Nair</a>, <a href="https://shikharbahl.github.io/">Shikhar Bahl</a>, <a href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a>
                  <br>
                  <em> International Conference on Robotics and Automation (ICRA), 2020. </em>  <font color="red"></font>
                  <br>
                  [<a href="https://arxiv.org/abs/2003.02636">arXiv</a>]
                  [<a href="https://sites.google.com/view/scalable-mili">site</a>]
                  [<a href="https://www.youtube.com/watch?v=JJlDdsrE83Q">video</a>]
                </p>
                <!-- <p> While robot learning has demonstrated promising results for enabling robots to automatically acquire new skills, a critical challenge in deploying learning-based systems is scale: acquiring enough data for the robot to effectively generalize broadly. Imitation learning, in particular, has remained a stable and powerful approach for robot learning, but critically relies on expert operators for data collection. In this work, we target this challenge, aiming to build an imitation learning system that can continuously improve through autonomous data collection, while simultaneously avoiding the explicit use of reinforcement learning, to maintain the stability, simplicity, and scalability of supervised imitation. To accomplish this, we cast the problem of imitation with autonomous improvement into a multi-task setting. We utilize the insight that, in a multi-task setting, a failed attempt at one task might represent a successful attempt at another task. This allows us to leverage the robot's own trials as demonstrations for tasks other than the one that the robot actually attempted. Using an initial dataset of multi-task demonstration data, the robot autonomously collects trials which are only sparsely labeled with a binary indication of whether the trial accomplished any useful task or not. We then embed the trials into a learned latent space of tasks, trained using only the initial demonstration dataset, to draw similarities between various trials, enabling the robot to achieve one-shot generalization to new tasks. In contrast to prior imitation learning approaches, our method can autonomously collect data with sparse supervision for continuous improvement, and in contrast to reinforcement learning algorithms, our method can effectively improve from sparse, task-agnostic reward signals.
                </p> -->
                <br>
                </p>
              </td>
          </tr>

          <tr>
              <td width="25%"><img src="images/rig_vitch.png" width="200"  style="border-style: none">
                <td width="75%" valign="top">
                  <p>
                    <a href="http://papers.nips.cc/paper/8132-visual-reinforcement-learning-with-imagined-goals.pdf">
                      <papertitle>Visual Reinforcement Learning with Imagined Goals</papertitle>
                    </a>
                    <br>
                    <a href="http://ashvin.me/">Ashvin Nair*</a>, <a href="http://people.eecs.berkeley.edu/~vitchyr/">Vitchyr H. Pong*</a>, <strong>Murtaza Dalal</strong>, <a href="https://shikharbahl.github.io/">Shikhar Bahl</a>, Steven Lin, <a href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a>
                    <br>
                    <em> Advances in Neural Information Processing Systems (NeurIPS), 2018 </em>  <font color="red"><strong>(Spotlight Presentation)</strong></font>
                    <br>
                    [<a href="https://arxiv.org/pdf/1807.04742.pdf">arXiv</a>]
                    [<a href="https://sites.google.com/site/visualrlwithimaginedgoals/">site</a>]
                    [<a href="https://github.com/vitchyr/rlkit">code</a>]
                    [<a href="https://bair.berkeley.edu/blog/2018/09/06/rig/">blog</a>]
                  </p>
                  <!-- <p> For an autonomous agent to fulfill a wide range of user-specified goals at test time, it must be able to learn broadly applicable and general-purpose skill repertoires. Furthermore, to provide the requisite level of generality, these skills must handle raw sensory input such as images. In this paper, we propose an algorithm that acquires such general-purpose skills by combining unsupervised representation learning and reinforcement learning of goal-conditioned policies. Since the particular goals that might be required at test-time are not known in advance, the agent performs a self-supervised "practice" phase where it imagines goals and attempts to achieve them. We learn a visual representation with three distinct purposes: sampling goals for self-supervised practice, providing a structured transformation of raw sensory inputs, and computing a reward signal for goal reaching. We also propose a retroactive goal relabeling scheme to further improve the sample-efficiency of our method. Our off-policy algorithm is efficient enough to learn policies that operate on raw image observations and goals for a real-world robotic system, and substantially outperforms prior techniques.
                  </p> -->
                  <br>
                  </p>
                </td>
            </tr>
            <tr>
              <td width="25%"><img src="images/composable.png" width="200" style="border-style: none"></td>
              <td width="75%" valign="top">
              <p>
              <a href="https://arxiv.org/abs/1803.06773">
                <papertitle>
                    Composable Deep Reinforcement Learning for Robotic Manipulation
                </papertitle>
                </a>
                <br>
                <a href="https://people.eecs.berkeley.edu/~haarnoja/">Tuomas Haarnoja*</a>,
                <a href="http://people.eecs.berkeley.edu/~vitchyr/">Vitchyr H. Pong</a>,
                Aurick Zhou,
                <strong>Murtaza Dalal</strong>,
                <a href="https://people.eecs.berkeley.edu/~pabbeel/">PieterAbbeel</a>,
                <a href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a>
                <br>
                <em>
                    International Conference on Robotics and Automation (ICRA), 2018.
                </em>
                <br>
                [<a href="https://arxiv.org/pdf/1803.06773.pdf">arXiv</a>]
                [<a href="https://sites.google.com/view/composing-real-world-policies">site</a>]
                [<a href="https://github.com/haarnoja/softqlearning/">code</a>]
              </p>
              <!-- <p>
              Model-free deep reinforcement learning has been shown to exhibit good
              performance in domains ranging from video games to simulated robotic
              manipulation and locomotion. However, model-free methods are known to
              perform poorly when the interaction time with the environment is
              limited, as is the case for most real-world robotic tasks. In this
              paper, we study how maximum entropy policies trained using soft
              Q-learning can be applied to real-world robotic manipulation. The
              application of this method to real-world manipulation is facilitated by
              two important features of soft Q-learning. First, soft Q-learning can
              learn multimodal exploration strategies by learning policies represented
              by expressive energy-based models. Second, we show that policies learned
              with soft Q-learning can be composed to create new policies, and that
              the optimality of the resulting policy can be bounded in terms of the
              divergence between the composed policies.
              </p> -->
              </td>
            </tr>
            <tr>
              <td width="25%"><img src="images/ant-smaller-2.gif" width="200" style="border-style: none"></td>
              <td width="75%" valign="top">
              <p>
              <a href="https://arxiv.org/pdf/1802.09081.pdf">
                <papertitle>Temporal Difference Models: Model-Free Deep RL for Model-Based Control</papertitle>
                </a>
                <br>
                <a href="http://people.eecs.berkeley.edu/~vitchyr/">Vitchyr H. Pong*</a>,
                <a href="http://sg717.user.srcf.net/">Shixiang Gu*</a>,
                <strong>Murtaza Dalal</strong>,
                <a href="https://people.eecs.berkeley.edu/~svlevine/">
                    Sergey Levine
                </a>
                <br>
                <em>
                    International Conference on Learning Representations (ICLR), 2018.
                </em>
                <br>
                [<a href="https://arxiv.org/abs/1802.09081">arXiv</a>]
                [<a href="https://github.com/vitchyr/rlkit">code</a>]
                [<a href="https://bair.berkeley.edu/blog/2018/04/26/tdm/">blog</a>]
              </p>
              <!-- <p>
              Model-free reinforcement learning (RL) is a powerful, general tool for
              learning complex behaviors. However, its sample efficiency is often
              impractical large for solving challenging real-world problems, even with 
              off-policy algorithms such as Q-learning.
              We introduce temporal difference models (TDMs), a family of
              goal-conditioned value functions that can be trained
              with model-free learning and used for model-based control.
              TDMs combine the benefits of model-free and model-based RL: they
              leverage the rich information in state transitions to learn very
              efficiently, while still attaining asymptotic performance that exceeds
              that of direct model-based RL methods.
              </p> -->
              </td>
            </tr>

        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td>
              <heading>Teaching</heading>
            </td>
          </tr>
        </table>
        <table width="100%" align="center" border="0" cellpadding="20">
          <tr>
            <td width="25%"><img src="images/cs188_bot.png" width="160" height="160"></td>
            <td width="75%" valign="center">
              <p>
                <a href="https://inst.eecs.berkeley.edu/~cs188/sp19/">
                  <papertitle>CS188 - Spring 2019 (uGSI)</papertitle>
                </a>
              </p>
            </td>
          </tr>
        </table>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td>
              <br>
              <p align="right">
                <font size="2">
                    Credit to this great <a href="https://github.com/jonbarron/jonbarron_website"><strong>repo</strong></a> for providing the source code for the website!
                  </font>
              </p>
            </td>
          </tr>
        </table>
        <script type="text/javascript">
          var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
          document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
        </script>
        <script type="text/javascript">
          try {
            var pageTracker = _gat._getTracker("UA-7580334-1");
            pageTracker._trackPageview();
          } catch (err) {}
        </script>
        </td>
    </tr>
  </table>
</body>

</html>
